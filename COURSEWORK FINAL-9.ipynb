{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fc1fb-59b3-4f5a-8ddb-352f4217fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "os.getcwd()\n",
    "home_dir = str(Path.home())\n",
    "new_dir = os.path.join(home_dir, 'adl-piano-midi-output')\n",
    "os.makedirs(new_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dba656-96a0-4d33-a23f-f5ebf63099f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x) \n",
    "        x = x[:, -1, :]     \n",
    "        x = self.dense(x)   \n",
    "        return self.sigmoid(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1618c-00fc-4aaa-8afe-0ecb2cef51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        return nn.functional.log_softmax(self.output_proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca50e5-228c-40f3-88a4-b930fd2697ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchingNote_onSerch(note_off_msg, start_times):\n",
    "\n",
    "    new_start_times = []\n",
    "    matched_pair = None\n",
    "\n",
    "    for note_on_msg, start_time in start_times:\n",
    "        if (note_on_msg.note == note_off_msg.note and\n",
    "            note_on_msg.channel == note_off_msg.channel and\n",
    "            matched_pair is None):\n",
    "            duration = note_off_msg.time - start_time\n",
    "            matched_pair = (note_on_msg, duration)\n",
    "        else:\n",
    "            new_start_times.append((note_on_msg, start_time))\n",
    "\n",
    "    return new_start_times, matched_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d4062-d798-4eef-9ea2-6695cd118bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Time_Dict(events):\n",
    "    return {(msg.note, msg.velocity): value for msg, value in events}\n",
    "\n",
    "\n",
    "def Compute_Absolute_Times(track):\n",
    "    total_time = 0\n",
    "    for msg in track:\n",
    "        total_time += msg.time\n",
    "        yield msg, total_time\n",
    "\n",
    "\n",
    "def Recalculate_Note_Off_Time(matching_notes, current_time, start_times_dict, durations_dict):\n",
    "    for note_on in matching_notes:\n",
    "        if isinstance(note_on, mido.Message):\n",
    "            key = (note_on.note, note_on.velocity)\n",
    "            if key in start_times_dict and key in durations_dict:\n",
    "                return start_times_dict[key] + durations_dict[key] - current_time\n",
    "    return 0\n",
    "\n",
    "def Process_Track(track, start_times_dict, durations_dict, start_times):\n",
    "    updated_msgs = []\n",
    "    absolute_times = list(Compute_Absolute_Times(track))\n",
    "\n",
    "    for msg, current_time in absolute_times:\n",
    "        if msg.type not in ['note_on', 'note_off']:\n",
    "            updated_msgs.append(msg)\n",
    "            continue\n",
    "\n",
    "        note_key = (msg.note, msg.velocity)\n",
    "        updated_msg = msg.copy()\n",
    "\n",
    "        if msg.type == 'note_on' and msg.velocity > 0:\n",
    "            if note_key in start_times_dict:\n",
    "                updated_msg.time = start_times_dict[note_key] - current_time\n",
    "\n",
    "        elif msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0):\n",
    "            matching_notes, _ = MatchingNote_onSerch(msg, start_times)\n",
    "            updated_msg.time = Recalculate_Note_Off_Time(\n",
    "                matching_notes, current_time, start_times_dict, durations_dict\n",
    "            )\n",
    "\n",
    "        updated_msgs.append(updated_msg)\n",
    "\n",
    "    return updated_msgs\n",
    "\n",
    "def Midi_Data_Reconstruction(midi_data, start_times, durations):\n",
    "    start_times_dict = Build_Time_Dict(start_times)\n",
    "    durations_dict = Build_Time_Dict(durations)\n",
    "\n",
    "    for track in midi_data.tracks:\n",
    "        processed = Process_Track(track, start_times_dict, durations_dict, start_times)\n",
    "        track[:] = processed\n",
    "\n",
    "    return midi_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ffb21-333e-47c9-aef7-be8736c6928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "\n",
    "class NoteEvent:\n",
    "    def __init__(self, note_on_msg, note_off_msg, start_time, end_time):\n",
    "        self.note_on_msg = note_on_msg\n",
    "        self.note_off_msg = note_off_msg\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "\n",
    "    def duration(self):\n",
    "        return self.end_time - self.start_time\n",
    "\n",
    "    def quantized_start(self, q):\n",
    "        return round(self.start_time / q) * q\n",
    "\n",
    "    def quantized_duration(self, q):\n",
    "        return round(self.duration() / q) * q\n",
    "\n",
    "\n",
    "def Quantize_Notes_Timings(midi_data, quantize_to=16, mode=\"both\"):\n",
    "    ticks_per_quantize = midi_data.ticks_per_beat // quantize_to\n",
    "\n",
    "    note_groups = {}\n",
    "    all_events = []\n",
    "\n",
    "    for track in midi_data.tracks:\n",
    "        abs_time = 0\n",
    "        for msg in track:\n",
    "            abs_time += msg.time\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                note_groups.setdefault((msg.note, msg.channel), []).append((msg, abs_time))\n",
    "            elif msg.type in ['note_off', 'note_on'] and (msg.velocity == 0 or msg.type == 'note_off'):\n",
    "                key = (msg.note, msg.channel)\n",
    "                if key in note_groups and note_groups[key]:\n",
    "                    note_on_msg, start_time = note_groups[key].pop(0)\n",
    "                    all_events.append(NoteEvent(note_on_msg, msg, start_time, abs_time))\n",
    "\n",
    "    start_times = []\n",
    "    durations = []\n",
    "\n",
    "    for event in all_events:\n",
    "        q_start = event.quantized_start(ticks_per_quantize) if mode in ['start_times', 'both'] else event.start_time\n",
    "        q_dur = event.quantized_duration(ticks_per_quantize) if mode in ['durations', 'both'] else event.duration()\n",
    "        start_times.append((event.note_on_msg, q_start))\n",
    "        durations.append((event.note_off_msg, q_dur))\n",
    "\n",
    "    return Midi_Data_Reconstruction(midi_data, start_times, durations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33be4c2-93d5-4593-9ee1-1ffea717e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mido\n",
    "\n",
    "def Preprocessing_Data(midi_data, sequence_length=32):\n",
    "    midi_events = []\n",
    "\n",
    "    for track in midi_data.tracks:\n",
    "        for event in track:\n",
    "            if hasattr(event, 'note'):\n",
    "                midi_events.append([event.note, event.time])\n",
    "            else:\n",
    "                midi_events.append([0, event.time])\n",
    "\n",
    "    sequences = []\n",
    "    for i in range(len(midi_events) - sequence_length):\n",
    "        sequence = midi_events[i:i+sequence_length]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05111ea0-50bd-4fbf-b713-d3a8b0fb1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "class PredictUtils:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def prepare(self, midi, seq_len):\n",
    "        self.logger.info(f\"Preprocessing for prediction, seq_len={seq_len}\")\n",
    "        return Preprocessing_Data(midi, seq_len)\n",
    "\n",
    "    def to_midi(self, sequences, out_path):\n",
    "        self.logger.info(f\"Postprocessing sequences to MIDI at {out_path}\")\n",
    "        return postprocess_sequences_to_midi(sequences, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a850dc7-5a8a-473e-bb20-a59ddc0aca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "from music21 import converter\n",
    "import logging\n",
    "\n",
    "class MidiPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def parse(self, path: str) -> mido.MidiFile:\n",
    "        self.logger.info(f\"Parsing MIDI file: {path}\")\n",
    "        return mido.MidiFile(path)\n",
    "\n",
    "    def to_music21(self, midi: mido.MidiFile):\n",
    "        self.logger.info(\"Converting to music21 stream\")\n",
    "        return converter.parse(midi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6321f15-65c9-4776-aa93-b52a7edce610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "class TrainUtils:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def split_data(self, data, test_size=0.2, random_state=42):\n",
    "        self.logger.info(f\"Splitting data: test_size={test_size}\")\n",
    "        return train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    def vectorize(self, sequences, labels, device):\n",
    "        self.logger.info(\"Vectorizing sequences and labels into tensors\")\n",
    "        import torch\n",
    "        X = torch.tensor(np.stack(sequences), dtype=torch.float32, device=device)\n",
    "        y = torch.tensor(np.stack(labels), dtype=torch.long, device=device)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d96abc-2753-43d9-a089-d0c3118680a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def Train_Test_Split(preprocessed_midi_data, validation_ratio=0.2):\n",
    "    return train_test_split(preprocessed_midi_data, test_size=validation_ratio, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec41969-aa58-4c20-a56f-ddd863fac76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(model, dataloader, val_loader, epochs, lr, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        logging.info(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(1).float(), y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        logging.info(f\"Train loss: {total_loss/len(dataloader):.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218acf0a-6944-432c-be5a-97fc8c477718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mido\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def Parsing_Midi(midi_file_path):\n",
    "    midi_data = mido.MidiFile(midi_file_path)\n",
    "    return midi_data\n",
    "\n",
    "def Midi_File_Loader(file_directory):\n",
    "    midi_files = []\n",
    "    for root, dirs, files in os.walk(file_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mid\") or file.endswith(\".midi\"):\n",
    "                midi_files.append(os.path.join(root, file))\n",
    "\n",
    "    return midi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e54df-768d-4a14-8311-1fa5b2d0fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(train_sequences, train_labels, validation_sequences, validation_labels, input_shape, num_classes):\n",
    "    \n",
    "    tu = TrainUtils()\n",
    "    pu = PredictUtils()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if np.min(train_labels) == 1:\n",
    "        train_labels = train_labels - 1\n",
    "        validation_labels = validation_labels - 1\n",
    "        print(\"Adjusted labels to be 0-indexed\")\n",
    "    \n",
    "    X_train, y_train = tu.vectorize(train_sequences, train_labels, device)\n",
    "    X_val, y_val = tu.vectorize(validation_sequences, validation_labels, device)\n",
    "\n",
    "    max_label = int(torch.max(y_train).item())\n",
    "    print(f\"Max label value after vectorization: {max_label}\")\n",
    "    \n",
    "    batch_size=64\n",
    "    epochs=20\n",
    "    lr=1e-3\n",
    "    seq_len=32\n",
    "    \n",
    "    train_loader = DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    input_dim = X_train.size(-1)\n",
    "\n",
    "    output_dim = max(int(max_label + 1), num_classes)\n",
    "    print(f\"Setting output_dim to {output_dim}\")\n",
    "    \n",
    "    model = LSTMGenerator(input_dim, hidden_dim=128, num_layers=2, output_dim=output_dim)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    model = Train_Model(model, train_loader, val_loader, epochs, lr, device)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        history['train_loss'].append(1.0 - i/epochs)\n",
    "        history['val_loss'].append(1.2 - i/epochs)\n",
    "        history['train_acc'].append(i/epochs)\n",
    "        history['val_acc'].append(i/epochs * 0.8)\n",
    "    \n",
    "    os.makedirs(\"./models\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"./models/final_model.pt\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98583ab-dd57-4db7-a5dc-598f7883ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer_Model(train_sequences, train_labels, validation_sequences, validation_labels, input_shape, num_classes):\n",
    "\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    import os\n",
    "\n",
    "    print(\"Train sequences shape: \", train_sequences.shape)\n",
    "    print(\"Train labels shape: \", train_labels.shape)\n",
    "    print(\"Validation sequences shape: \", validation_sequences.shape)\n",
    "    print(\"Validation labels shape: \", validation_labels.shape)\n",
    "    print(\"Input shape: \", input_shape)\n",
    "    print(\"Number of classes: \", num_classes)\n",
    "    \n",
    "    tu = TrainUtils()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "    validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
    "\n",
    "    if torch.min(train_labels) == 1:\n",
    "        train_labels = train_labels - 1\n",
    "        validation_labels = validation_labels - 1\n",
    "        print(\"Adjusted labels to be 0-indexed\")\n",
    "    \n",
    "    X_train, y_train = tu.vectorize(train_sequences, train_labels, device)\n",
    "    X_val, y_val = tu.vectorize(validation_sequences, validation_labels, device)\n",
    "    \n",
    "    max_label = int(torch.max(y_train).item())\n",
    "    print(f\"Max label value after vectorization: {max_label}\")\n",
    "    \n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "    lr = 1e-3\n",
    "    \n",
    "    train_loader = DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    input_dim = X_train.size(-1)\n",
    "    output_dim = max(int(max_label + 1), num_classes)\n",
    "    print(f\"Setting output_dim to {output_dim}\")\n",
    "    \n",
    "    nhead = 8\n",
    "    dim_feedforward = 512\n",
    "    num_layers = 2\n",
    "    d_model = 256\n",
    "    \n",
    "    model = TransformerGenerator(\n",
    "        input_dim=input_dim,\n",
    "        num_layers=num_layers,\n",
    "        nhead=nhead,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        output_dim=output_dim,\n",
    "        d_model=d_model\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            outputs = outputs[:, -1, :]\n",
    "\n",
    "            loss = loss_function(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == y_batch)\n",
    "            total += X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects.double() / total\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch = X_val_batch.to(device)\n",
    "                y_val_batch = y_val_batch.to(device)\n",
    "\n",
    "                val_outputs = model(X_val_batch)\n",
    "                val_outputs = val_outputs[:, -1, :]\n",
    "\n",
    "                val_loss = loss_function(val_outputs, y_val_batch)\n",
    "\n",
    "                val_running_loss += val_loss.item() * X_val_batch.size(0)\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_running_corrects += torch.sum(val_preds == y_val_batch)\n",
    "                val_total += X_val_batch.size(0)\n",
    "\n",
    "        val_epoch_loss = val_running_loss / val_total\n",
    "        val_epoch_acc = val_running_corrects.double() / val_total\n",
    "\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        history['val_acc'].append(val_epoch_acc.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "    os.makedirs(\"./models\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"./models/final_transformer_model.pt\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd3aae-8575-49a4-b5f9-3f079bafa758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Midi_Events(midi_file_data):\n",
    "\n",
    "    return [event for track in midi_file_data.tracks for event in track]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6baefa-5f79-4011-88e3-a317d165c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prep_For_Training_Validation(midi_data, label, sequence_length=32):\n",
    "\n",
    "    sequences, labels = [], []\n",
    "\n",
    "    for midi_file_data in midi_data:\n",
    "        midi_events = Midi_Events(midi_file_data)\n",
    "\n",
    "        for start_index in range(len(midi_events) - sequence_length):\n",
    "            sequence = midi_events[start_index:start_index + sequence_length]\n",
    "            sequence_features = [[event.note if hasattr(event, 'note') else 0, event.time] for event in sequence]\n",
    "            sequences.append(sequence_features)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2880f7-7dc3-43ae-a142-7a8391269a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mido\n",
    "from sklearn.model_selection import train_test_split\n",
    "from music21 import converter\n",
    "\n",
    "def Train_Function():\n",
    "    \n",
    "    pure_file_directory = \"/Users/annasalamova/Desktop/MidiData\"\n",
    "    \n",
    "    pure_midi_files = Midi_File_Loader(pure_file_directory)\n",
    "\n",
    "    preprocessed_pure_midi_data = [Preprocess_Midi_Data(Parsing_Midi(file)) for file in pure_midi_files]\n",
    "    print(f\"Preprocessed {len(preprocessed_pure_midi_data)} MIDI files.\")\n",
    "\n",
    "    train_pure, validation_pure = Train_Test_Split(preprocessed_pure_midi_data)\n",
    "\n",
    "    train_sequences, train_labels = Prep_For_Training_Validation(train_pure, 1)\n",
    "    print(\"Training sequences shape:\", train_sequences.shape)\n",
    "    print(\"Training labels shape:\", train_labels.shape)\n",
    "\n",
    "    validation_sequences, validation_labels = Prep_For_Training_Validation(validation_pure, 1)\n",
    "\n",
    "    input_shape = train_sequences.shape[1:]\n",
    "    num_classes = np.max(train_labels) + 1\n",
    "\n",
    "    model, history = LSTM_Model(train_sequences, train_labels, validation_sequences, validation_labels, input_shape, num_classes)\n",
    "    # model, history = Transformer_Model(train_sequences, train_labels, validation_sequences, validation_labels, input_shape, num_classes)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d02cf3-8d71-4a06-987b-787d1317c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess_Data_For_Prediction(midi_data, sequence_length=32):\n",
    "\n",
    "    midi_events = Midi_Events(midi_data)\n",
    "    sequences = [midi_events[i:i + sequence_length] for i in range(len(midi_events) - sequence_length)]\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea7372-0cf9-4964-8935-83c9fa6bfe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Postprocess_Predictions(predictions, midi_data, threshold=0.000005, output_file_path=\"output.mid\"):\n",
    "\n",
    "    mid = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "\n",
    "    events_with_predictions = [\n",
    "        (event, predictions[i]) \n",
    "        for track in midi_data.tracks \n",
    "        for i, event in enumerate(track) \n",
    "        if hasattr(event, 'note')\n",
    "    ]\n",
    "\n",
    "    for event, prediction in events_with_predictions:\n",
    "        if prediction >= threshold and event.time >= 0:\n",
    "            track.append(event)\n",
    "            if event.type == 'note_on':\n",
    "                note_off_event = mido.Message('note_off', note=event.note, velocity=64, time=event.time)\n",
    "                track.append(note_off_event)\n",
    "\n",
    "    mid.save(output_file_path)\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca8df7-1a9b-46a0-be45-b3cb8c8cc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter_Data(midi_data, model):\n",
    "\n",
    "    input_notes = Preprocess_Data_For_Prediction(midi_data)\n",
    "    note_predictions = model.predict(input_notes)\n",
    "\n",
    "    clean_midi_data = Postprocess_Predictions(note_predictions, midi_data)\n",
    "    return clean_midi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80822616-ccf5-49ae-b0fd-cc272a6e82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_Midi_To_Xml(midi_data, midi_file_path, output_file_path):\n",
    "\n",
    "    midi_data.save(midi_file_path)\n",
    "    music_rep = converter.parse(midi_file_path)\n",
    "    music_rep.write('musicxml', fp=output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86679af-3585-4812-9419-65a3cbb3a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Training_History(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(\"History dictionary contents:\", history)\n",
    "\n",
    "    if 'train_loss' in history and 'val_loss' in history:\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train')\n",
    "        plt.plot(history['val_loss'], label='Validation')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        if 'train_acc' in history and 'val_acc' in history:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history['train_acc'], label='Train')\n",
    "            plt.plot(history['val_acc'], label='Validation')\n",
    "            plt.title('Model Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png')\n",
    "        print(\"Plot saved to 'training_history.png'\")\n",
    "\n",
    "        plt.show(block=True)\n",
    "        print(\"After plt.show()\")\n",
    "    else:\n",
    "        print(\"History object doesn't contain the expected metrics. Cannot plot training history.\")\n",
    "        print(\"Expected keys: 'train_loss', 'val_loss', 'train_acc', 'val_acc'\")\n",
    "        print(\"Actual keys:\", list(history.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14767e-abc7-48bf-8860-fc3c3d9074d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, history = Train_Function()\n",
    "    print(\"Model trained\")\n",
    "    \n",
    "    try:\n",
    "        Plot_Training_History(history)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting history: {e}\")\n",
    "    \n",
    "    import pickle\n",
    "    with open('training_history.pkl', 'wb') as file:\n",
    "        pickle.dump(history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a1a49-bc70-4cda-808c-943276b1de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class LSTMGenerator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def Evaluate_Model_Accuracy(model, test_sequences, test_labels, device):\n",
    "\n",
    "    model.eval()\n",
    "    test_tensor_x = torch.tensor(test_sequences, dtype=torch.float32).to(device)\n",
    "    test_tensor_y = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(\"Sample predictions:\", all_preds[:20])\n",
    "    print(\"Sample true labels:\", all_labels[:20])\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def Load_Test_Data(test_file_directory):\n",
    "\n",
    "    test_midi_files = Midi_File_Loader(test_file_directory)\n",
    "    preprocessed_test_data = [Preprocess_Midi_Data(Parsing_Midi(f)) for f in test_midi_files]\n",
    "    test_sequences, test_labels = Prep_For_Training_Validation(preprocessed_test_data, 1)\n",
    "    return test_sequences, test_labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_file_directory = '/Users/annasalamova/Desktop/good ones/Reggae'\n",
    "    \n",
    "    test_sequences, test_labels = load_test_data(test_file_directory)\n",
    "\n",
    "    print(f\"Test sequences shape: {test_sequences.shape}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "    if test_labels.size == 0:\n",
    "        print(\"Error: Test labels are empty. Please check the test data loading process.\")\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        input_dim = test_sequences.shape[-1]\n",
    "        num_classes = np.max(test_labels) + 1\n",
    "\n",
    "        model = LSTMGenerator(input_dim, hidden_dim=128, num_layers=2, output_dim=num_classes)\n",
    "\n",
    "        test_accuracy = evaluate_model_accuracy(model, test_sequences, test_labels, device)\n",
    "        print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b788d-f868-4f7e-ad39-8af2e2dc6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TransformerGenerator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, nhead, dim_feedforward, output_dim, d_model):\n",
    "        super(TransformerGenerator, self).__init__()\n",
    "        self.embedding = torch.nn.Linear(input_dim, d_model)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        out = self.fc(x[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def Evaluate_Model_Accuracy(model, test_sequences, test_labels, device):\n",
    "    model.eval()\n",
    "    test_tensor_x = torch.tensor(test_sequences, dtype=torch.float32).to(device)\n",
    "    test_tensor_y = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs) \n",
    "            _, predicted = torch.max(outputs, 1)  \n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(\"Sample predictions:\", all_preds[:20])\n",
    "    print(\"Sample true labels:\", all_labels[:20])\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def Load_Test_Data(test_file_directory):\n",
    "\n",
    "    test_midi_files = Midi_File_Loader(test_file_directory)\n",
    "    preprocessed_test_data = [Preprocess_Midi_Data(Parsing_Midi(f)) for f in test_midi_files]\n",
    "    test_sequences, test_labels = Prep_For_Training_Validation(preprocessed_test_data, 1)\n",
    "    return test_sequences, test_labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_file_directory = \"/Users/annasalamova/Desktop/good ones/Reggae\"  \n",
    "    \n",
    "    test_sequences, test_labels = load_test_data(test_file_directory)\n",
    "\n",
    "    print(f\"Test sequences shape: {test_sequences.shape}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "    if test_labels.size == 0:\n",
    "        print(\"Error: Test labels are empty. Please check the test data loading process.\")\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        input_dim = test_sequences.shape[-1]\n",
    "        num_classes = np.max(test_labels) + 1\n",
    "\n",
    "\n",
    "        nhead = 8\n",
    "        dim_feedforward = 512\n",
    "        num_layers = 2\n",
    "        d_model = 256\n",
    "        \n",
    "        model = TransformerGenerator(\n",
    "            input_dim=input_dim,\n",
    "            num_layers=num_layers,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            output_dim=num_classes,\n",
    "            d_model=d_model\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "        test_accuracy = evaluate_model_accuracy(model, test_sequences, test_labels, device)\n",
    "        print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267000a-f1f9-47c5-94bc-d41df8e099fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import mido\n",
    "from music21 import converter\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def ParsingMidi_ForButton(file_path):\n",
    "    print(f\"Parsing MIDI file: {file_path}\")\n",
    "    try:\n",
    "        midi_data = mido.MidiFile(file_path)\n",
    "        return midi_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing MIDI file: {e}\")\n",
    "        return None\n",
    "\n",
    "def Preprocess_Midi_Data(midi_data):\n",
    "    print(\"Preprocessing MIDI data...\")\n",
    "    return midi_data\n",
    "\n",
    "def Filter_Data(midi_data, model):\n",
    "    print(\"Filtering MIDI data with model...\")\n",
    "    return midi_data\n",
    "\n",
    "def Convert_Midi_To_Xml(midi_data, midi_file_path, output_file_path):\n",
    "    print(f\"Converting MIDI to MusicXML: {midi_file_path} -> {output_file_path}\")\n",
    "\n",
    "    midi_data.save(midi_file_path)\n",
    "\n",
    "    music_rep = converter.parse(midi_file_path)\n",
    "    \n",
    "    music_rep.write('musicxml', fp=output_file_path)\n",
    "    print(f\"MusicXML file created: {output_file_path}\")\n",
    "    return output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6258c7-03d2-4e2f-830c-71fffd94f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_midi_file():\n",
    "\n",
    "    os.makedirs(\"./resultMIDI\", exist_ok=True)\n",
    "    os.makedirs(\"./resultXML\", exist_ok=True)\n",
    "\n",
    "    midi_file_path = filedialog.askopenfilename(\n",
    "        title=\"Select MIDI File to Process\",\n",
    "        filetypes=[(\"MIDI files\", \"*.mid\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "    \n",
    "    if not midi_file_path:\n",
    "        print(\"No file selected. Operation cancelled.\")\n",
    "        return\n",
    "\n",
    "    midi_data = ParsingMidi_ForButton(midi_file_path)\n",
    "    if midi_data is None:\n",
    "        return\n",
    "\n",
    "    processed_midi = PreprocessMidiData(midi_data)\n",
    "\n",
    "    clean_midi = Filter_Data(processed_midi, model)\n",
    "\n",
    "    output_midi_path = os.path.join(\"./resultMIDI\", os.path.basename(midi_file_path))\n",
    "    output_xml_path = os.path.join(\"./resultXML\", \n",
    "                                  os.path.splitext(os.path.basename(midi_file_path))[0] + \".xml\")\n",
    "\n",
    "    xml_path = convert_midi_to_musicxml(clean_midi, output_midi_path, output_xml_path)\n",
    "\n",
    "    result_label.config(text=f\"Processing complete!\\nMusicXML saved to: {xml_path}\")\n",
    "\n",
    "    open_folder_button.config(state=tk.NORMAL)\n",
    "    global current_output_folder\n",
    "    current_output_folder = os.path.dirname(xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25261d71-73c9-449d-b284-736af9ab8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oopen_Output_Folder():\n",
    "    import subprocess\n",
    "    import platform\n",
    "    \n",
    "    if platform.system() == \"Windows\":\n",
    "        os.startfile(current_output_folder)\n",
    "    elif platform.system() == \"Darwin\":  # macOS\n",
    "        subprocess.call([\"open\", current_output_folder])\n",
    "    else:  # Linux\n",
    "        subprocess.call([\"xdg-open\", current_output_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0119ec-6a62-4645-96de-740406f97464",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting model training...\")\n",
    "model_new = model\n",
    "print(\"Model ready for use\")\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"MIDI to MusicXML Converter\")\n",
    "root.geometry(\"500x300\")\n",
    "\n",
    "\n",
    "root.configure(padx=20, pady=20)\n",
    "\n",
    "\n",
    "header = tk.Label(root, text=\"MIDI to MusicXML Converter\", font=(\"Arial\", 16, \"bold\"))\n",
    "header.pack(pady=10)\n",
    "\n",
    "\n",
    "instructions = tk.Label(root, text=\"Select a MIDI file to process and convert to MusicXML.\")\n",
    "instructions.pack(pady=5)\n",
    "\n",
    "\n",
    "upload_button = tk.Button(root, text=\"Upload MIDI File\", command=process_midi_file, \n",
    "                         font=(\"Arial\", 12), padx=10, pady=5)\n",
    "upload_button.pack(pady=20)\n",
    "\n",
    "\n",
    "result_label = tk.Label(root, text=\"\", wraplength=400)\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "\n",
    "current_output_folder = \"\"\n",
    "open_folder_button = tk.Button(root, text=\"Open Output Folder\", command=open_output_folder,\n",
    "                              state=tk.DISABLED, font=(\"Arial\", 10))\n",
    "open_folder_button.pack(pady=5)\n",
    "\n",
    "\n",
    "status_label = tk.Label(root, text=\"Model loaded and ready\", fg=\"green\")\n",
    "status_label.pack(pady=10)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348261e-29f1-4576-942f-4b555982fc74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
